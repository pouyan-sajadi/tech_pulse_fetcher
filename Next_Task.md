# Phase 2: Integrating `Tech_pulse_monitor` with Supabase

**Goal:** Automate the daily execution of the `Tech_pulse_monitor` script, saving its JSON output into our Supabase database. We will then create a secure API endpoint in `Signal App` to fetch this data for the frontend.

---

### Step 1: Create the Database Table in Supabase

First, we need a place in our database to store the daily data. We'll create a table specifically for this.

1.  Navigate to your Supabase project dashboard.
2.  In the left sidebar, click the **Table Editor** icon.
3.  In the main panel, click **+ New table**.
4.  Alternatively, for more control, go to the **SQL Editor** icon in the sidebar. Click **+ New query** and run the following SQL command. This is the **recommended** method.

    ```sql
    CREATE TABLE public.tech_pulses (
      id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
      created_at TIMESTAMPTZ DEFAULT NOW() NOT NULL,
      pulse_data JSONB NOT NULL
    );

    -- Optional: Add a comment to describe the table's purpose
    COMMENT ON TABLE public.tech_pulses IS 'Stores the daily JSON output from the Tech Pulse Monitor script.';
    ```
5.  Click **RUN**. This creates a `tech_pulses` table with three columns:
    *   **`id`**: A unique number for each entry.
    *   **`created_at`**: A timestamp that automatically records when the data was inserted.
    *   **`pulse_data`**: A highly efficient and queryable `JSONB` column to hold our generated JSON.

---

### Step 2: Modify the `Tech_pulse_monitor` Script

Now, we'll update your data-fetching script to write to this new table instead of a local file. This example assumes your script is a **Node.js** project.

1.  **Install a PostgreSQL client library:** In your `Tech_pulse_monitor` repository's terminal, run:
    ```bash
    npm install pg
    ```

2.  **Update your main script file** (e.g., `index.js`, `main.js`). The new logic will connect to the database and perform an `INSERT`.

    ```javascript
    // index.js in Tech_pulse_monitor repo
    const { Client } = require('pg');

    // This is a placeholder for your existing data fetching logic.
    // It must return the final JSON object you want to save.
    async function fetchAndProcessData() {
      console.log('Fetching and processing data from 4 tech sources...');
      // ... your actual fetching and processing logic here ...
      const finalJson = {
        timestamp: new Date().toISOString(),
        headlines: [
          { source: 'TechCrunch', title: 'New AI Model Released' },
          { source: 'HackerNews', title: 'Show HN: My New Project' },
        ],
        market_trends: {
          ai_stocks: 'up'
        }
      };
      console.log('Data processing complete.');
      return finalJson;
    }

    // This new function saves the data to our Supabase Postgres database.
    async function saveDataToSupabase(data) {
      // The client will automatically use environment variables for connection.
      // PGHOST, PGPORT, PGDATABASE, PGUSER, PGPASSWORD
      const client = new Client();

      try {
        console.log('Connecting to the database...');
        await client.connect();
        console.log('Database connection successful.');

        const query = 'INSERT INTO public.tech_pulses(pulse_data) VALUES($1)';
        // We pass the JSON data as a string. Postgres will handle the conversion to JSONB.
        const values = [JSON.stringify(data)];

        await client.query(query, values);
        console.log('Data successfully saved to the database.');

      } catch (err) {
        console.error('Error saving data to database:', err);
        // Throw the error to make sure the GitHub Action fails if the DB write fails.
        throw err;
      } finally {
        await client.end();
        console.log('Database connection closed.');
      }
    }

    // Main execution block
    async function main() {
      try {
        const processedData = await fetchAndProcessData();
        await saveDataToSupabase(processedData);
        console.log('Tech Pulse Monitor script completed successfully.');
      } catch (error) {
        console.error('An error occurred during the script execution:', error);
        process.exit(1); // Exit with an error code
      }
    }

    main();
    ```

---

### Step 3: Configure Database Credentials for the Script

Your script needs credentials to connect to Supabase.

1.  In your Supabase project dashboard, go to **Project Settings** (gear icon) -> **Database**.
2.  Under **Connection string**, select the **URI** tab.
3.  Your URI will look like this: `postgresql://postgres:[YOUR-PASSWORD]@[AWS-HOST]:[PORT]/postgres`.
4.  You will need to create **secrets** in GitHub for your `Tech_pulse_monitor` repo with these values. **Do not hardcode them in your script.**

---

### Step 4: Create a Daily Scheduler with GitHub Actions

We will use GitHub Actions to run your script automatically every day.

1.  In your `Tech_pulse_monitor` repository, create a new directory structure: `.github/workflows/`.
2.  Inside that folder, create a new file named `daily-pulse.yml`.
3.  Paste the following code into `daily-pulse.yml`:

    ```yaml
    name: Run Daily Tech Pulse Monitor

    on:
      # Runs once a day at 1 AM UTC. You can customize the time.
      schedule:
        - cron: '0 1 * * *'
      # Allows you to run this workflow manually from the Actions tab
      workflow_dispatch:

    jobs:
      run-pulse-monitor:
        runs-on: ubuntu-latest

        steps:
          - name: Check out repository code
            uses: actions/checkout@v4

          - name: Set up Node.js
            uses: actions/setup-node@v4
            with:
              node-version: '18' # Or your preferred Node.js version

          - name: Install dependencies
            run: npm install

          - name: Run the pulse monitor script
            # These environment variables are read by the 'pg' client
            env:
              PGHOST: ${{ secrets.DB_HOST }}
              PGPORT: ${{ secrets.DB_PORT }}
              PGDATABASE: ${{ secrets.DB_DATABASE }}
              PGUSER: ${{ secrets.DB_USER }}
              PGPASSWORD: ${{ secrets.DB_PASSWORD }}
            run: node index.js # Or the name of your main script file
    ```

### Step 5: Configure GitHub Secrets

1.  In your `Tech_pulse_monitor` GitHub repository, go to **Settings** -> **Secrets and variables** -> **Actions**.
2.  Click the **New repository secret** button for each secret below.
3.  Create the following five secrets using the values from your Supabase connection string:
    *   `DB_HOST`: The host address (e.g., `aws-0-us-west-1.pooler.supabase.com`).
    *   `DB_PORT`: The port number (e.g., `5432`).
    *   `DB_DATABASE`: The database name (usually `postgres`).
    *   `DB_USER`: The user (usually `postgres`).
    *   `DB_PASSWORD`: The database password you saved when creating the project.

Now, your GitHub Action is fully configured to run daily and securely connect to your database.

---

### Step 6: Build the API Endpoint in `Signal App`

Finally, let's create the API in your Next.js app to read the latest data.

1.  In your `Signal App` project, create a new API route file at:
    `app/api/tech-pulse/latest/route.ts`

2.  Add the following code to the `route.ts` file.

    ```typescript
    // app/api/tech-pulse/latest/route.ts

    import { createClient } from '@supabase/supabase-js';
    import { NextResponse } from 'next/server';

    // It's safe to use these env vars here because this code ONLY runs on the server.
    const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;
    const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;

    // Create a Supabase client for server-side operations
    const supabaseAdmin = createClient(supabaseUrl, supabaseServiceKey);

    // Revalidate this route on-demand or every hour, for example.
    // export const revalidate = 3600;

    export async function GET() {
      try {
        const { data, error } = await supabaseAdmin
          .from('tech_pulses')
          .select('pulse_data, created_at')
          .order('created_at', { ascending: false }) // Get the newest first
          .limit(1)  // We only want the most recent one
          .single(); // Expect a single row response

        if (error) {
          // If .single() finds no rows, it returns an error. Handle this gracefully.
          if (error.code === 'PGRST116') {
            return NextResponse.json({ message: 'No tech pulse data found.' }, { status: 404 });
          }
          throw error;
        }

        return NextResponse.json(data);

      } catch (err: any) {
        console.error('Error fetching latest tech pulse:', err);
        return NextResponse.json(
          { message: 'Error fetching latest tech pulse', error: err.message },
          { status: 500 }
        );
      }
    }

    ```

**Completion:** You have now fully connected your data pipeline. Your frontend can now call `fetch('/api/tech-pulse/latest')` to get the latest daily data, which is served instantly from your database.